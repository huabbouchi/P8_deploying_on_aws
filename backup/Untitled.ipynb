{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e7bb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    " \n",
    "# image\n",
    "from PIL import Image\n",
    "import io\n",
    "from io import StringIO\n",
    "from skimage.io import imread, imshow\n",
    "\n",
    "import cv2\n",
    "\n",
    "# S3 AWS\n",
    "import boto3\n",
    "import configparser\n",
    "\n",
    "# Spark\n",
    "import findspark  #Findspark : Make Spark available in Jupyter notebook\n",
    "findspark.init('/home/ubuntu/spark-3.3.0-bin-hadoop3')\n",
    "\n",
    "# Pyspark.\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import input_file_name, udf, col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, DoubleType, DataType, FloatType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, array_to_img\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d73cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the aws credentials in order to be able to access the s3 bucket. \n",
    "# We can use the configparser package to read the credentials from the standard aws file.\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "aws_credentials = 'default'\n",
    "AWS_ACCESS_KEY_ID = config.get(aws_credentials, \"aws_access_key_id\") \n",
    "AWS_SECRET_ACCESS_KEY = config.get(aws_credentials, \"aws_secret_access_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a294e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/config\"))\n",
    "aws_config = 'default'\n",
    "RGION_NAME = config.get(aws_config, \"region\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d957f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets:\n",
      "  ocr-projet8-fruits\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the list of existing buckets\n",
    "session = boto3.session.Session(aws_access_key_id=AWS_ACCESS_KEY_ID, \n",
    "                                aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3_client = session.client(service_name='s3', region_name= RGION_NAME)\n",
    "\n",
    "# Output the bucket names\n",
    "response = s3_client.list_buckets()\n",
    "\n",
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'  {bucket[\"Name\"]}')\n",
    "BUCKET_NAME = response['Buckets'][0][\"Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "367dac69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://ocr-projet8-fruits/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration chemins S3\n",
    "DATASET_PATH = 's3://' + BUCKET_NAME + '/'\n",
    "DIR_PATH = 'Training/'\n",
    "# DIR_PATH = 'Test_s3/'\n",
    "DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baf65662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "575\n",
      "[{'Prefix': 'Training/apple_6/'}, {'Prefix': 'Training/apple_braeburn_1/'}, {'Prefix': 'Training/apple_crimson_snow_1/'}, {'Prefix': 'Training/apple_golden_1/'}, {'Prefix': 'Training/apple_golden_3/'}, {'Prefix': 'Training/apple_granny_smith_1/'}, {'Prefix': 'Training/apple_hit_1/'}, {'Prefix': 'Training/apple_pink_lady_1/'}, {'Prefix': 'Training/apple_red_1/'}, {'Prefix': 'Training/apple_red_2/'}, {'Prefix': 'Training/apple_red_3/'}, {'Prefix': 'Training/apple_red_yellow_1/'}, {'Prefix': 'Training/apple_rotten_1/'}, {'Prefix': 'Training/cabbage_white_1/'}, {'Prefix': 'Training/carrot_1/'}, {'Prefix': 'Training/cucumber_1/'}, {'Prefix': 'Training/cucumber_3/'}, {'Prefix': 'Training/eggplant_violet_1/'}, {'Prefix': 'Training/pear_1/'}, {'Prefix': 'Training/pear_3/'}, {'Prefix': 'Training/zucchini_1/'}, {'Prefix': 'Training/zucchini_dark_1/'}]\n"
     ]
    }
   ],
   "source": [
    "# Create a reusable Paginator\n",
    "paginator = s3_client.get_paginator('list_objects')\n",
    "\n",
    "# Create a PageIterator from the Paginator\n",
    "page_iterator = paginator.paginate(Bucket=BUCKET_NAME)\n",
    "\n",
    "for page in page_iterator:\n",
    "    print(len(page['Contents']))\n",
    "\n",
    "ld_set_folders = s3_client.list_objects_v2(Bucket=BUCKET_NAME,\n",
    "                                           Prefix=DIR_PATH, Delimiter='/')\n",
    "print(ld_set_folders.get('CommonPrefixes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a8676e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://ocr-projet8-fruits/Training/Training/apple_6/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_braeburn_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_crimson_snow_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_golden_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_golden_3/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_granny_smith_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_hit_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_pink_lady_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_red_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_red_2/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_red_3/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_red_yellow_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/apple_rotten_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/cabbage_white_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/carrot_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/cucumber_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/cucumber_3/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/eggplant_violet_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/pear_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/pear_3/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/zucchini_1/*',\n",
       " 's3://ocr-projet8-fruits/Training/Training/zucchini_dark_1/*']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_folders = []\n",
    "result = s3_client.list_objects(Bucket=BUCKET_NAME, Prefix=DIR_PATH, Delimiter='/')\n",
    "for o in result.get('CommonPrefixes'):\n",
    "    #print ('sub folder : ', o.get('Prefix'))\n",
    "    l= DATASET_PATH + DIR_PATH + o.get('Prefix') + '*'\n",
    "    path_folders.append(l)\n",
    "path_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e39cf5",
   "metadata": {},
   "source": [
    "### Enable access to s3 data from Spark\n",
    "\n",
    "In order to be able to read data via S3A we need a couple of dependencies / \n",
    "we need to make sure the hadoop-aws and aws-java-sdk packages are available when we load spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9c85b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add an environnement variable\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:3.3.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95fe56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "             .builder.master('local[*]')\n",
    "             .appName('fruits')\n",
    "             .getOrCreate()\n",
    "            )\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f89ce9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-45-51.eu-west-3.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>fruits</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc9d71aa160>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66fd0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "zipped = zip(path_folders)\n",
    "columns = ['path_folders']\n",
    "\n",
    "df_images = spark.createDataFrame(zipped, columns)\n",
    "df_images.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90825973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(path_folders)\n",
    "df = rdd.toDF([\"path_folders\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fe270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
