{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b4ec88-0ba8-4b87-9e10-e7c051a36d71",
   "metadata": {},
   "source": [
    "# Notebook mis sur le serveur EC2\n",
    "## avec récupération des images sur un serveur S3\n",
    "## puis sauvegarde des résultats du préprocessing sur le S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80efbe2e-5a55-46b0-a090-ec277542d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.3.0-bin-hadoop3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c5411a-7a7f-4dba-84b8-21c6259c6d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 16:57:24.975131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-04 16:57:25.379934: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-04 16:57:26.140152: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-04 16:57:26.140205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-04 16:57:26.140210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "import configparser # Nécessaire pour retouver les codes AWS\n",
    "import boto3 # Pour la communication avec S3\n",
    "import os # Pour la variable d'environnement \"PYSPARK_SUBMIT_ARGS\"\n",
    "from io import StringIO # Nécessaire pour l'export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cf84e0-5066-41e2-bec5-53f23ff22b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout d'une variable d'environnement\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:3.3.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aec2e6c-58cf-4a67-953c-1689604cccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 16:57:28 WARN Utils: Your hostname, Franck-ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.1.116 instead (on interface enxc025a5dc297d)\n",
      "22/10/04 16:57:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/agent/Logiciels/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/agent/.ivy2/cache\n",
      "The jars for the packages stored in: /home/agent/.ivy2/jars\n",
      "com.amazonaws#aws-java-sdk-pom added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-23bb8d8c-6655-421f-a42b-338896dc20ea;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazonaws#aws-java-sdk-pom;1.10.34 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 155ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-pom;1.10.34 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-23bb8d8c-6655-421f-a42b-338896dc20ea\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 16:57:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.116:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>p8</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f641209bfd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création d'une session spark\n",
    "spark = (SparkSession\n",
    "             .builder.master('local[*]')\n",
    "             .appName('p8')\n",
    "             .config('spark.executor.memory', '14g')\n",
    "             .config('spark.network.timeout', '1000000') \n",
    "             .getOrCreate()\n",
    "            )\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26d6fc-65d9-4315-b5c5-103450a90e55",
   "metadata": {},
   "source": [
    "## Configuration communication avec S3 et récupération des photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34334d3-d9df-48a9-a7b9-0aa10b0e26d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des acces AWS S3 (fonctionne car awscli installé et fichier ~/.aws/credentials présent)\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "aws_profile = 'default'\n",
    "access_id = config.get(aws_profile, \"aws_access_key_id\") \n",
    "access_key = config.get(aws_profile, \"aws_secret_access_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd37920e-c7d4-418f-bd4f-2802d1e8374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration chemins S3\n",
    "DIR_PATH = 'Training/'\n",
    "# DIR_PATH = 'Test_s3/'\n",
    "REGION_NAME = 'eu-west-1'\n",
    "BUCKET_NAME = 'oc-p8'\n",
    "\n",
    "dataset_path = 's3://' + BUCKET_NAME + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211b8055-1a61-4a9c-8a65-dac6b2562364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création session S3\n",
    "session = boto3.session.Session(aws_access_key_id=access_id, aws_secret_access_key=access_key)\n",
    "s3_client = session.client(service_name='s3', region_name=REGION_NAME)\n",
    "\n",
    "# Liste de toutes les photos du répertoire (list_objets_v2 limite à 1000 images)\n",
    "# https://stackoverflow.com/questions/54314563/how-to-get-more-than-1000-objects-from-s3-by-using-list-objects-v2\n",
    "liste_s3 = s3_client.list_objects_v2(Bucket=BUCKET_NAME, Prefix=DIR_PATH)\n",
    "len(liste_s3['Contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b461d76a-f331-495e-83ac-471e9dc01d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une liste avec les photos\n",
    "liste_photos = []\n",
    "\n",
    "for file in liste_s3['Contents']:\n",
    "    photo = file['Key']\n",
    "    liste_photos.append(photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe928c0-320f-4d75-9c55-d3a8793177c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                path|\n",
      "+--------------------+\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "|Training/apple_6/...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Définition d'un dataframe spark avec la liste des photos\n",
    "\n",
    "zipped = zip(liste_photos)\n",
    "cols = ['path']\n",
    "\n",
    "df_images = spark.createDataFrame(zipped, cols)\n",
    "df_images.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70b9504-0756-4e7f-9cf9-de1f80228ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classe(path):\n",
    "    '''\n",
    "    Défini la classe du fruit en fonction du nom de répertoire\n",
    "    '''\n",
    "    classe_detaillee = path.split('/')[-2] # Nom du repertoire complet\n",
    "    classe_fruit = classe_detaillee.split('_')[0] #Nom de la classe\n",
    "    return classe_fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d596cd03-a1ff-43b1-ad89-ecfcb146e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                path|classe|\n",
      "+--------------------+------+\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "|Training/apple_6/...| apple|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ajout de la classe en tant que colonne du dataframe\n",
    "udf_fruits = udf(find_classe, StringType())\n",
    "df_images = df_images.withColumn('classe', udf_fruits('path'))\n",
    "df_images.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb86beb-3351-4dde-910d-3170a3c8ae38",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eccb747-c64e-4d1a-adcd-cbf6753b6e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 16:57:35.766428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-04 16:57:35.843682: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-04 16:57:35.843700: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-04 16:57:35.844452: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 512)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle avec suppression de la dernière couche et remplacement avec une couche de pooling\n",
    "model_vgg = VGG16(weights=\"imagenet\", include_top=False, pooling='max', input_shape=(224, 224, 3))\n",
    "print(model_vgg.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f2fb1ba-6b63-46f2-bf5f-f4d5a72728ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://df6eac6c-846b-4bc0-9557-9aea752b2a9f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://df6eac6c-846b-4bc0-9557-9aea752b2a9f/assets\n"
     ]
    }
   ],
   "source": [
    "# Sérialise le modèle pour une execution plus rapide\n",
    "sc = spark.sparkContext\n",
    "model_bc = sc.broadcast(model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e5fe176-d13d-4d65-be74-76644f564461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Création des features VGG\n",
    "# Pas réussi avec le dataframe spark donc on repasse par une session S3\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    REGION_NAME,\n",
    "                    aws_access_key_id=access_id,\n",
    "                    aws_secret_access_key=access_key)\n",
    "bucket = s3.Bucket('oc-p8')\n",
    "\n",
    "vgg_features=[]\n",
    "\n",
    "for photo in liste_s3['Contents']:\n",
    "    file = photo['Key']\n",
    "    obj = bucket.Object(file)\n",
    "    img_body = obj.get()['Body'] # Méthode pour récupérer les photos\n",
    "    \n",
    "    img = Image.open(img_body).resize((224, 224))\n",
    "    img_arr = np.expand_dims(img, axis=0)\n",
    "    features = model_bc.value.predict(img_arr,verbose=0)\n",
    "    vector_feature = Vectors.dense(features.ravel().tolist())    \n",
    "    vgg_features.append(vector_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce6bbcbe-5010-413f-b2c2-76082138896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dataframe spark temporaire pour récuérer les photos\n",
    "temp_vgg = spark.createDataFrame([(f,) for f in vgg_features], ['VGG_features'])\n",
    "\n",
    "# Ajout des colonnes index pour préparer la jointure\n",
    "df_images = df_images.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "temp_vgg = temp_vgg.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "# Jointure des 2 dataframes\n",
    "df_images = df_images.join(temp_vgg, df_images.row_index == temp_vgg.row_index).drop('row_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16e7e942-2e7b-400a-b0d4-664c73bff16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation des features\n",
    "to_vector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "df_images_vgg = df_images.select('path', 'classe', to_vector('VGG_features').alias('VGG_features'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106a0cc-4ff5-42c1-b2a4-6196bb6d8e59",
   "metadata": {},
   "source": [
    "## Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37fc6255-f132-40e3-9b43-426067541b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|                path|classe|     features_scaled|\n",
      "+--------------------+------+--------------------+\n",
      "|Training/apple_6/...| apple|[0.13037561896527...|\n",
      "|Training/apple_6/...| apple|[-0.1536597553272...|\n",
      "|Training/apple_6/...| apple|[-0.4919848263588...|\n",
      "|Training/apple_6/...| apple|[-0.4908032568120...|\n",
      "|Training/apple_6/...| apple|[-0.2817242142361...|\n",
      "|Training/apple_6/...| apple|[-0.2731925101659...|\n",
      "|Training/apple_6/...| apple|[0.12042080215325...|\n",
      "|Training/apple_6/...| apple|[0.00159375186577...|\n",
      "|Training/apple_6/...| apple|[0.40183381419129...|\n",
      "|Training/apple_6/...| apple|[0.35310259910108...|\n",
      "|Training/apple_6/...| apple|[0.41057468968808...|\n",
      "|Training/apple_6/...| apple|[0.01257500274985...|\n",
      "|Training/apple_6/...| apple|[0.79925479351586...|\n",
      "|Training/apple_6/...| apple|[1.20735223107430...|\n",
      "|Training/apple_6/...| apple|[0.87150011413529...|\n",
      "|Training/apple_6/...| apple|[0.85806682630126...|\n",
      "|Training/apple_6/...| apple|[0.04572892288315...|\n",
      "|Training/apple_6/...| apple|[0.37238745759488...|\n",
      "|Training/apple_6/...| apple|[-0.2804425412208...|\n",
      "|Training/apple_6/...| apple|[0.05076721511512...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Standardisation des données\n",
    "standardizer = StandardScaler(withMean=True, withStd=True,\n",
    "                              inputCol='VGG_features',\n",
    "                              outputCol='features_scaled')\n",
    "std = standardizer.fit(df_images_vgg)\n",
    "df_image_std = std.transform(df_images_vgg)\n",
    "df_image_std = df_image_std.select('path', 'classe', 'features_scaled')\n",
    "\n",
    "df_image_std.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06542f89-9cb3-414c-8201-4f904c5d54e7",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ac5dec7-7171-481b-834d-3beddc8b1e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|                path|classe|        Features_PCA|\n",
      "+--------------------+------+--------------------+\n",
      "|Training/apple_6/...| apple|[10.1501684291780...|\n",
      "|Training/apple_6/...| apple|[8.17683279765479...|\n",
      "|Training/apple_6/...| apple|[10.0884364030640...|\n",
      "|Training/apple_6/...| apple|[8.66049061761517...|\n",
      "|Training/apple_6/...| apple|[8.41705675960248...|\n",
      "|Training/apple_6/...| apple|[7.57242713155589...|\n",
      "|Training/apple_6/...| apple|[9.03474106119812...|\n",
      "|Training/apple_6/...| apple|[9.22834823859929...|\n",
      "|Training/apple_6/...| apple|[8.77502046560159...|\n",
      "|Training/apple_6/...| apple|[8.95114979715183...|\n",
      "|Training/apple_6/...| apple|[8.89578226750046...|\n",
      "|Training/apple_6/...| apple|[9.18700034422483...|\n",
      "|Training/apple_6/...| apple|[8.58090088847377...|\n",
      "|Training/apple_6/...| apple|[9.85677766824411...|\n",
      "|Training/apple_6/...| apple|[9.51503747037526...|\n",
      "|Training/apple_6/...| apple|[9.63721351710998...|\n",
      "|Training/apple_6/...| apple|[9.23456205832679...|\n",
      "|Training/apple_6/...| apple|[8.96674822949604...|\n",
      "|Training/apple_6/...| apple|[9.13721732742973...|\n",
      "|Training/apple_6/...| apple|[9.54648316394086...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "pca = PCA(k=512, inputCol=\"features_scaled\", outputCol=\"Features_PCA\")\n",
    "pca_model = pca.fit(df_image_std)\n",
    "\n",
    "result = pca_model.transform(df_image_std).select('path', 'classe', 'Features_PCA')\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6966a-5e07-415c-81d3-b31492e7cf9a",
   "metadata": {},
   "source": [
    "## Export du fichier vers S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f38512ec-ccd5-4e72-8248-0f9bac6962c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '527YG9PMNEB379RZ',\n",
       "  'HostId': '1zEmchfxk3o6exeWxy6BYcCtnKr8by61OI3/ydD9b7iFOKY2dbfnP2yHbELRrjz5yWf48eOg3og=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '1zEmchfxk3o6exeWxy6BYcCtnKr8by61OI3/ydD9b7iFOKY2dbfnP2yHbELRrjz5yWf48eOg3og=',\n",
       "   'x-amz-request-id': '527YG9PMNEB379RZ',\n",
       "   'date': 'Tue, 04 Oct 2022 15:01:08 GMT',\n",
       "   'etag': '\"7d21eeddfaef59a269c3cd0a07ddf233\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"7d21eeddfaef59a269c3cd0a07ddf233\"'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_buffer = StringIO()\n",
    "\n",
    "result_df = result.toPandas()\n",
    "result_df.to_csv(csv_buffer)\n",
    "\n",
    "s3.Object(BUCKET_NAME, 'output/result_df.csv').put(Body=csv_buffer.getvalue()) # Sauvegarde du fichier csv sur S3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
